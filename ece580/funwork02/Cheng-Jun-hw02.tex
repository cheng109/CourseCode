\documentclass{article}

% Packages required to support encoding
\usepackage{ucs}
\usepackage[utf8x]{inputenc}
\usepackage{graphicx} 
% Packages required by code

% Packages always used
\usepackage{listings}
\usepackage{hyperref}
\usepackage{xspace}
\usepackage[usenames,dvipsnames]{color}
\hypersetup{colorlinks=true,urlcolor=blue}


\usepackage[framed,numbered,autolinebreaks,useliterate] {mcode}

\input{homework.tex}


\begin{document} 


\subsection*{Problem 1: }

\begin{align*} 
f(x_1, x_2) &= x_1^3 + 2x_1x_2 - 3x_1^2x_2^2 \\
\frac{df}{dx_1} &= 2x_1^2 + 2x_2 - 6x_1x_2^2 \\
\frac{df}{dx_2} & = 2x_1 -6x_1^2x_2   \\
f_{x_1x_1} & = 4x_1 -6x_2^2 \\
f_{x_1x_2} & = 2 - 12x_1x_2  \\
f_{x_2x_1} & = 2 - 12x_1x_2  \\
f_{x_2x_2} & = -6x_1^2 \\
\end{align*}

Tylor expansion: \\
\begin{align*} 
f(\vx) = f(\vx^0) + (\vx-\vx^0)^T\bigtriangledown f(\vx^0) + (\vx-\vx^0)^TH(\vx)(\vx-\vx^0) \\
\end{align*} 
where $\vx^0 = \bmat{1 \\ 1 }  $ \\
For linear approximation: 

\begin{align*} 
l(x_1, x_2) &  =  f(\vx^0) + (\vx-\vx^0)^T\bigtriangledown f(\vx^0) \\
& = f\left( \bmat{1\\ 1} \right) + \bmat{x_1-1 & x_2-1}\bmat{-1\\-4} \\
& = -x_1 -4x_2 + 5 \\
\end{align*}
Hessian: \\
\begin{align*} 
H = \bmat{f_{x_1x_1} & f_{x_1x_2} \\  f_{x_2x_1} & f_{x_2x_2} } 
\end{align*} 
For quadratic approximation: 
\begin{align*}
f(\vx) & = f(\vx^0) + (\vx-\vx^0)^T\bigtriangledown f(\vx^0) + (\vx-\vx^0)^TH(\vx)(\vx-\vx^0) \\
& = -x_1 -4x_2 + 5 + \bmat{x_1-1 & x_2 -1}\bmat{4x_1 -6x_2^2 & 2 - 12x_1x_2 \\ 2 - 12x_1x_2 & -6x_1^2 } \bmat{x_1-1 \\ x_2 -1 }   \\
& = -x_1 -4x_2 + 5  + (2x_1 - 6x_1^2x_2)(x_1-1)^2 - 6x_1^2(x_2-1)^2 + (4-24x_1x_2)(x_1-1)(x_2-1) \\ 
\end{align*}

\subsection*{Problem 2: } 

\begin{enumerate} 
\item  
\begin{align*}
f(\vx)  & = \bmat{x_1 & x_2} \bmat{1 & 4 \\ 2 & 3} \bmat{x_1 // x_2} - \bmat{x_1 & x_2} \bmat{-2 \\ 1} + \pi \\ 
& = x_1^2 +3 x_2^2 + 6x_1x_2 + 2x_1 + x_2 + \pi \\
\frac{df}{dx_1} &= 2x_1 + 6x_2 + 2 \\
\frac{df}{dx_2} &= 6x_2 + 6x_1 + 1 \\
Df(\vx) &= \bmat{\frac{df}{dx_1} & \frac{df}{dx_2}} = \bmat{2x_1 + 6x_2 + 2 &   6x_2 + 6x_1 + 1} 
\end{align*} 
\item 
\begin{align*} 
f(\vx) & = \frac{1}{2} \bmat{x_1 & x_2 } \bmat{2 & 3 \\ 9 & 1} \bmat{x_1\\ x_2} + \bmat {x_1 & x_2} \bmat {2 \\ -3 }  + \log{3}  \\
& = x_1^2 +  \frac{1}{2}x_2^2 + 6x_1x_2  + 2x_1 - 3x_2 + \log{3} \\ 
\frac{df}{x_1} & = 2x_1 + 6x_2 + 2 \\
\frac{df}{x_2} & = x_2+6x_1 - 3 \\
f_{x_1x_1} &  = 2 \\ 
f_{x_1x_2} &  = 6 \\ 
f_{x_2x_1} &  = 6 \\ 
f_{x_2x_2} &  = 1 \\ 
\end{align*}
Hessian: \\
\begin{align*} 
H = \bmat{2 & 6 \\ 6 & 1} 
\end{align*} 
\end{enumerate} 


\subsection*{Problem 3: } 

\begin{align*} 
f(x_1, x_2) = e^{3x_1x_2^2} 
\end{align*} 
\begin{enumerate} 
\item 
\begin{align*} 
\frac{df}{x_1} &= 3x_2^2e^{3x_1x_2^2}\\
\frac{df}{x_2} &= 6x_1x_2e^{3x_1x_2^2}\\ 
\end{align*} 
Then the gradient at $\vx = \bmat{1 \\ 1} $ is : \\
\begin{align*}
\bmat{3e^3 \\ 6 e^3 } 
\end{align*} 

\item 
Rate of increase of $f$ at the point $\vx=\bmat{1\\1} $  in the direction $\vd$ 
\begin{align*} 
\frac{Df(\vx)\cdot \vd}{\| \vd \|} = \frac{15e^3}{\sqrt{5}} 
\end{align*} 

\item 
In the direction $\vd = \bmat{3 \\ 6 }$, we can have the maximum rate of increase $3\sqrt{5} e^3$ 


\end{enumerate} 


\subsection*{Problem 4: } 
\begin{align*} 
f(x_1, x_2, x_3) &  =  -( x_1^2 + 4\epsilon x_2^2 + 5 x_3^2 -2x_1x_3 + 2\epsilon x_1x_2 + 4 x_2 x_3 )   \\
& =- \bmat{x_1 & x_2 & x_3 } \bmat{1 & \epsilon & -1 \\ \epsilon & 4\epsilon & 2 \\ -1 & 2 & 5} \\
\end{align*}
%Eigenvalues of $ \bmat{1 & \epsilon & -1 \\ \epsilon & 4\epsilon & 2 \\ -1 & 2 & 5} $  are: \\
Using principal minor method: \\ 
\begin{align*} 
k= 1: \\
D_1 &= 20\epsilon - 4 > 0 \\
D_2 &= 7 \\
D_3 &= 4\epsilon - \epsilon^2 >0 \\
k=2 : \\
D_1 &= 1 \\
D_2 &= 4\epsilon>0  \\
D_3 &= 5 \\
k= 3 : \\
D &= 12\epsilon - 4 -5 \epsilon^2 > 0 \\
\end{align*} 

To make the function be negative semi-definite, the range of $\epsilon$ is $ 0.4 \leq \epsilon \leq 2 $ 

\subsection*{Problem 5: } 
\begin{enumerate} 
\item
\begin{align*}
f(x_1, x_2) & = \frac{1}{3}x_2^3 + \frac{1}{2}x_2^2 + 2x_1x_2 + \frac{1}{2}x_1^2-x_1+10 \\
\frac{df}{dx_1} &= 2x_2 + x_1 -1 = 0 \\
\frac{df}{dx_2} &= x_2^2 +x_2 + 2x_1 = 0 \\
\end{align*} 
We can get two points $(-1, 1)$ and $(-3, 2)$ which satisfy the first-order necessary conditions for the extremum. \\
\item 
\begin{align*} 
\frac{df}{dx_1dx_1} &= 1 \\
\frac{df}{dx_1dx_2} &=  2 \\
\frac{df}{dx_2dx_1} &= 2 \\
\frac{df}{dx_2dx_2} &= 2x_2 + 1 \\
\end{align*}
Then Hessian is : \begin{align*} \bmat{1 & 2 \\ 2 & 2x_2 + 1} \end{align*} \\
For point $(-1, 1)$ , \\
$H = \bmat {1 & 2 \\ 2 & 3} $  is indefinite
For point $(-3, 2) $ , \\
$H = \bmat{1 & 2 \\ 2 & 5} $ is positive definite
Then the point $(-3, 2)$ is a strict local minimizer. 

\end{enumerate} 

\subsection*{Problem 6: } 


\end{document}

