\documentclass{article}

% Packages required to support encoding
\usepackage{ucs}
\usepackage[utf8x]{inputenc}
\usepackage{graphicx} 
% Packages required by code

% Packages always used
\usepackage{listings}
\usepackage{hyperref}
\usepackage{xspace}
\usepackage[usenames,dvipsnames]{color}
\hypersetup{colorlinks=true,urlcolor=blue}


\usepackage[framed,numbered,autolinebreaks,useliterate] {mcode}
\newcommand{\BigO}[1]{\ensuremath{\operatorname{O}\bigl(#1\bigr)}}
\input{homework.tex}


\begin{document} 



\hypertarget{problem_0_homework_checklist_2}{}
\subsection*{{Problem 0: Homework checklist}}
\label{problem_0_homework_checklist_2}

\checkmark	I didn't talk with any one about this homework. \newline
\checkmark 	Source-code are included at the end of this document. 


\hypertarget{problem_0_homework_checklist_2}{}
\subsection*{{Problem 1: The Cholesky Factorization}}
\label{}
\begin{enumerate} 
\item 
Here is my implementation of Cholesky decomposition in Matlab: 
\begin{lstlisting}
function L = cholesky(A)
n = size(A); 
n = n(1); 
L = zeros(n,n);
%L(1,1) = sqrt(A(1,1)); 
for i=(1:n)
    sum1 =0;     
    for j=(1:i) 

        if i==j
            L(j,j) = sqrt(A(j,j)-sum1); 
        else 
            sum2 = 0; 
            for k = (1:j-1) 
                sum2 = sum2 + L(i,k)*L(j,k); 
            end 
            
            L(i,j) = 1/L(j,j)*(A(i,j)-sum2); 
        end 
        sum1= sum1 + L(i,j)^2;   
    end 
end 
end 
\end{lstlisting} 
\item 
Cholesky factorization is unique if $\mA$ is positive definite and the decomposition need not be unique when $\mA$ is positive semidefinite.  

\item 
Both have the same result. 
\item 
For each size of the matrices I repeated 10 times. The comparison result is shown.   Obviously the the Cholesky factorization has better performance as the growth of matrix size. 
\begin{figure} 
\centering 
\includegraphics[width=0.8\textwidth]{luVScholesky}
\caption{Performance comparison between LU decomposition and Cholesky factorization}
\end{figure}

\item 
Since $\mA$ is positive definite, and assume an vector $\vv = \bmat{x_0\\ \vx}, \\ $then \\
\begin{align*} 
\vv_T\mA\vv>0 \\
\bmat{x_0 & \vx}\bmat{\alpha & \vb_T\\ \vb & \mC} \bmat{x_0\\ \vx} = \alpha x_0^2 + x_0(\vx^T\vb+\vb^T\vx) + \vx^T\mC\vx >0  
\end{align*}
Then 
\begin{align*}
\vx^T(\mC-\frac{\vb\vb^T}{\alpha})\vx &= \vx^T\mC\vx - \frac{\vx_T\vb\vb^T\vx}{\alpha} \\
& > -\alpha x_0^2 - x_0(\vx^T\vb+\vb^T\vx) - \frac{\vx^T\vb\vb^T\vx}{\alpha} \\
& = -\frac{1}{\alpha} \left[x_0^2 + \frac{\vx^T\vb+\vb^T\vx)x_0}{\alpha}+\frac{\vx^T\vb\vb^T\vx}{\alpha^2}\right]\\
& = -\frac{1}{\alpha}\left(x_0+\frac{\vx^T\vb}{\alpha}\right)\left(x_0+\frac{\vb^T\vx}{\alpha}\right) \\
& =  -\frac{1}{\alpha}\left\|x_0+\frac{\vb^T\vx}{\alpha}\right\|^2 
\end{align*} 
Because $\vx^T(\mC-\frac{\vb\vb^T}{\alpha})\vx > -\frac{1}{\alpha}\left\|x_0+\frac{\vb^T\vx}{\alpha}\right\|^2$ holds for any vector $\vx$. Therefore \begin{align} 
\vx^T(\mC-\frac{\vb\vb^T}{\alpha})\vx > 0 
\end{align} 
So $\mC-\frac{\vb\vb^T}{\alpha}$ is positive definite. 

\item  
The base case is that $C$ is a $1\times1$. 

\item 
\end{enumerate}

\hypertarget{}{}
\subsection*{{Problem 2: Stability analysis}}
\label{}
\begin{enumerate} 
\item 

\item 

\end{enumerate} 

\hypertarget{}{}
\subsection*{{Problem 3: Backwards stability}}
\label{}

\begin{align*}
fl(x_i) = x_i(1+\epsilon_{ix}) \\
fl(y_i) = y_i(1+\epsilon_{iy}) \\
\end{align*} 
where $|\epsilon_{ix}|, |\epsilon_{iy}|\leq\epsilon_{machine} $ \\
\begin{align*}
\alpha &=\vx^T\vy \\
& = \bmat{x_1 & x_2 & .. & x_n}\bmat{y_1 \\ y_2 \\ .. \\ y_n} \\
& = \sum_{i=1}^nx_i y_i \\ 
\end{align*} 
Then \begin{align*} 
fl(\vx^T\vy) & = \sum_{i=1}^nfl(x_i)\times fl( y_i) \\
& = \left[fl(x_1)\times fl(y_1)+ fl(x_2)\times fl(y_2) + ... + fl(x_n)\times fl(y_n)\right] (1+\epsilon_{addition})\\
& = \left[x_1y_1(1+\epsilon_{1x})(1+\epsilon_{1y}) + ... + x_ny_n(1+\epsilon_{nx})(1+\epsilon_{ny})\right](1+\epsilon_{addition}) \\ 
& = \left[x_1y_1 + x_2y_2 + ... +x_ny_n\right](1+\epsilon_1)(1+\epsilon_2)(1+\epsilon_3)\\
& = \left[x_1y_1 + x_2y_2 + ... +x_ny_n\right](1+\BigO{\epsilon_{machine}^2})
\end{align*}
Therefore the inner product is backwards stable. \\

\end{document}
